<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Sepehr Kazemi</title>

    <meta name="author" content="Sepehr Kazemi">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Sepehr Kazemi
                </p>
                <p>I'm an undergraduate at Sharif University of Technology, Electrical Engineering department. I'm also working as a research assistant at <a href="https://www.l3s.de/">L3S Research Center</a>, working on explainability and bias & fairness in large vision language models. In L3S, I am so fortunate to work with <a href="https://www.kbs.uni-hannover.de/~nejdl/">Wolfgang Nejdl</a>, and <a href="https://cmp.felk.cvut.cz/~iscenahm/">Ahmet Iscen (Google DeepMind)</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:sepehrkazemi9@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/Sepehr_Kazemi_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=UUogPz0AAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/sepehrkr/">Github</a> &nbsp;/&nbsp;
		  <a href="https://www.linkedin.com/in/sepehr-kazemi-ranjbar-baa555242/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/SPProfile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/SPProfile.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
		  My general research theme lies at the intersection of machine learning, computer vision, and natural language processing. More specifically, I'm interested in
			<b>explainability and bias & fairness of large foundation models</b>, <b>language-informed reinforcement learning</b>, <b>knowledge graphs</b>, and <b>commonsense reasoning</b>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<h2>Publications</h2>
<p></p>
  <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/PO.jpg' width="160" class="hoverZoomLink">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openreview.net/group?id=ICLR.cc/2025/Conference/Authors&referrer=%5BHomepage%5D(%2F)">
			<span class="papertitle">Aligning Visual Contrastive Learning Models via Preference Optimization
</span>
        </a>
        <br>
	Amirabbas Afzali*,
	Borna Khodabandeh*,
	Ali Rasekh,
	Mahyar JafariNodeh,
	<b>Sepehr Kazemi Ranjbar</b>,
	Simon Gottschalk
	</br>
        <br>
        <em>arxiv</em>, 2024
        <br>
        <p></p>
        <p>
           This paper introduces a novel method for training contrastive learning models using Preference Optimization (PO)
		to break down complex concepts. Our method systematically aligns model behavior with desired preferences,
		enhancing performance on the targeted task.
        </p>
      </td>
   </tr>

  <p></p>		  
  <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/ExIQA.jpg' width="160" class="hoverZoomLink">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2409.06853">
			<span class="papertitle">ExIQA: Explainable Image Quality Assessment Using Distortion Attributes
</span>
        </a>
        <br>
	<b>Sepehr Kazemi Ranjbar</b>,
	Emad Fatemizadeh
	</br>
        <br>
        <em>arxiv</em>, 2024
        <br>
        <p></p>
        <p>
		In this paper, we approach BIQA from a distortion identification perspective,
		where our primary goal is to predict distortion types and strengths using Vision-Language Models (VLMs),
		such as CLIP, due to their extensive knowledge and generalizability. Based on these predicted distortions,
		we then estimate the quality score of the image.
        </p>
      </td>
   </tr>

  <p></p>	  
  <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/ECOR.jpg' width="160" class="hoverZoomLink">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2404.12839">
			<span class="papertitle">ECOR: Explainable CLIP for Object Recognition
</span>
        </a>
        <br>
	<b>Sepehr Kazemi Ranjbar</b>*,
	Ali Rasekh*,
	Milad Heidari,
	Wolfgang Nejdl
	</br>
        <br>
        <em>arxiv</em>, 2024
        <br>
        <p></p>
        <p>
		In this paper, we first propose a mathematical definition of explainability in the object recognition task based
		on the joint probability distribution of categories and rationales, 
		then leverage this definition to fine-tune CLIP in an explainable manner. 
        </p>
      </td>
   </tr>

   <p></p>		  
   <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/LLM_for_5G.jpg' width="160" class="hoverZoomLink">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.eurecom.fr/en/publication/7687">
			<span class="papertitle">LLM for 5G: Network Management
</span>
        </a>
        <br>
	Ali Mamaghani*,
        Ali Nourian*, 
        Negin Mohtaram*,
        Alireza Shokrani*, 
	Seyed Mohsen Nasiri*,
	<b>Sepehr Kazemi Ranjbar*</b>,
	Alireza Mohammadi,
	Navid Nikaein,
	Babak Hossein Khalaj
	</br>
        <br>
        <em>ICMLCN</em>, 2024
        <br>
        <p></p>
        <p>
		The demonstration comprises a user-friendly chatbot,
		adept at translating everyday English queries into actionable 5G
		commands, and LLMs serving as generative AIs to dynamically
		generate configurations tailored to the 5G network environment. 
        </p>
      </td>
    </tr>

		  
    </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
		  &copy;2024 Sepehr Kazemi. Powered by <a href="https://jonbarron.info/">jonbarron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
